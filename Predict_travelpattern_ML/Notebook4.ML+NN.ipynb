{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qp6Jzx49W_ta"
   },
   "source": [
    "# Prediction of customers' travel pattern\n",
    "\n",
    "- Applying ML + Neural Network models to selected featured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)-Importing key modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V11g1goXDyF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# For processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)\n",
    "plt.rcParams[\"xtick.labelsize\"] = 10\n",
    "plt.figure(figsize=(16,10)) # this creates a figure 16 inch wide, 10 inch high\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For modeling building and tunning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)-Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45805, 514)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('updated_feature.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>distance</th>\n",
       "      <th>num_family</th>\n",
       "      <th>len_jour</th>\n",
       "      <th>origin_ADB</th>\n",
       "      <th>origin_ADL</th>\n",
       "      <th>origin_AER</th>\n",
       "      <th>origin_AGP</th>\n",
       "      <th>origin_AKL</th>\n",
       "      <th>origin_ALA</th>\n",
       "      <th>...</th>\n",
       "      <th>dest_YEG</th>\n",
       "      <th>dest_YMQ</th>\n",
       "      <th>dest_YOW</th>\n",
       "      <th>dest_YTO</th>\n",
       "      <th>dest_YUL</th>\n",
       "      <th>dest_YVR</th>\n",
       "      <th>dest_YWG</th>\n",
       "      <th>dest_YYC</th>\n",
       "      <th>dest_YYZ</th>\n",
       "      <th>dest_ZRH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5834.154716</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6525.926149</td>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>469.781624</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1498.817537</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.339028</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_type     distance  num_family  len_jour  origin_ADB  origin_ADL  \\\n",
       "0           0  5834.154716           7       6.0           0           0   \n",
       "1           1  6525.926149           4      21.0           0           0   \n",
       "2           1   469.781624           2       3.0           0           0   \n",
       "3           1  1498.817537           1       3.0           0           0   \n",
       "4           1  2921.339028           4       6.0           0           0   \n",
       "\n",
       "   origin_AER  origin_AGP  origin_AKL  origin_ALA  ...  dest_YEG  dest_YMQ  \\\n",
       "0           0           0           0           0  ...         0         0   \n",
       "1           0           0           0           0  ...         0         0   \n",
       "2           0           0           0           0  ...         0         0   \n",
       "3           0           0           0           0  ...         0         0   \n",
       "4           0           0           0           0  ...         0         0   \n",
       "\n",
       "   dest_YOW  dest_YTO  dest_YUL  dest_YVR  dest_YWG  dest_YYC  dest_YYZ  \\\n",
       "0         0         0         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0         0   \n",
       "2         0         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0         0   \n",
       "\n",
       "   dest_ZRH  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 514 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[\"event_type\"]\n",
    "features=df.drop(['event_type'], axis=1)\n",
    "X = StandardScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "nm = NearMiss()\n",
    "X_under,y_under=nm.fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3616, 513) (3616,)\n"
     ]
    }
   ],
   "source": [
    "print(X_under.shape,y_under.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 43997, 1: 1808})\n",
      "Over-sampled dataset shape Counter({0: 1808, 1: 1808})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "print('Over-sampled dataset shape {}'.format(Counter(y_under)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)- ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append((\"NB\",GaussianNB()))\n",
    "models.append((\"SVM\",SVC()))\n",
    "models.append((\"KNN\",KNeighborsClassifier()))\n",
    "models.append((\"DT\",DecisionTreeClassifier()))\n",
    "models.append((\"RF\",RandomForestClassifier()))\n",
    "models.append((\"GB\",GradientBoostingClassifier()))\n",
    "models.append((\"MLP\",MLPClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.5024691358024691\n",
      "NB 0.5142857142857142\n",
      "SVM 0.5025000000000001\n",
      "KNN 0.5017391304347826\n",
      "DT 0.5008771929824561\n",
      "RF 0.5008230452674897\n",
      "GB 0.5011299435028248\n",
      "MLP 0.5015384615384615\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name,model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=22)\n",
    "    cv_result = cross_val_score(model,X_under,y_under, cv = kfold,scoring = \"precision\")\n",
    "    names.append(name)\n",
    "    results.append(cv_result)\n",
    "for i in range(len(names)):\n",
    "    print(names[i],results[i].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision results for these models are pretty same. We didnt get any improvement from Logistic Regress- being the simplest model to Random Forest, Gradiant Boosting or even multilayer perceptron- more advanced level models. Let's see how deep learning aka neural network with Stochastic gradient descent works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)- Implementing Neural Network\n",
    "Steps to follow\n",
    "\n",
    "- 1-Build the Neural Network\n",
    "- 2 -Set the hyperparameters, train the NN and evaluate\n",
    "- 3- Adapt SGD method to improve the accuracy & Recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=df[\"event_type\"]\n",
    "features=df.drop(['event_type'], axis=1)\n",
    "X = StandardScaler().fit_transform(features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3, random_state=0)\n",
    "nm = NearMiss()\n",
    "X_under,y_under=nm.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)-Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes =  input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.hidden_nodes,self.input_nodes))\n",
    "\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                       (self.output_nodes, self.hidden_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### Set this to your implemented sigmoid function ####\n",
    "        # Activation function is the sigmoid function\n",
    "        self.sigmoid_activation = lambda x : 1 / (1 + np.exp(-x))\n",
    "        self.sigmoid_output_2_derivative = lambda x: x * (1 - x)\n",
    "    \n",
    "    def train(self, inputs_array, targets_array):\n",
    "        # Convert inputs list to 2d array\n",
    "        inputs  = inputs_array.T\n",
    "        targets = np.array(targets_array, ndmin=2)\n",
    "        #targets = targets_array\n",
    "        m = inputs_array.shape[0] # number of records\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        ### Forward pass ###\n",
    "        # TODO: Hidden layer\n",
    "        layer_1_inputs = np.dot(self.weights_0_1, inputs) # signals into hidden layer\n",
    "        layer_1 = layer_1_inputs # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer\n",
    "        layer_2_inputs = np.dot(self.weights_1_2,layer_1) # signals into final output layer\n",
    "        layer_2 = self.sigmoid_activation(layer_2_inputs) # signals from final output layer\n",
    "        \n",
    "        #### Implement the backward pass here ####\n",
    "        ### Backward pass ###\n",
    "        \n",
    "        # TODO: Output error  \n",
    "        layer_2_errors = targets - layer_2  # Output layer error is the difference between desired target and actual output.\n",
    "        layer_2_delta = layer_2_errors * self.sigmoid_output_2_derivative(layer_2)\n",
    "        \n",
    "        # TODO: Backpropagated error\n",
    "        layer_1_errors = np.dot(self.weights_1_2.T,layer_2_delta) # errors propagated to the hidden layer 2x128\n",
    "        layer_1_delta = layer_1_errors  # hidden layer gradients y = x -> 1\n",
    "        \n",
    "        # TODO: Update the weights\n",
    "        self.weights_1_2 += self.lr*np.dot(layer_2_delta,layer_1.T)/m # update hidden-to-output weights with gradient descent step\n",
    "        self.weights_0_1 += self.lr*np.dot(layer_1_delta,inputs.T)/m # update input-to-hidden weights with gradient descent step\n",
    "         \n",
    "        \n",
    "    def run(self, inputs_list):\n",
    "        # Run a forward pass through the network\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer\n",
    "        hidden_inputs = np.dot(self.weights_0_1, inputs) # signals into hidden layer\n",
    "        hidden_outputs = hidden_inputs # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer\n",
    "        final_inputs = np.dot(self.weights_1_2,hidden_outputs) # signals into final output layer\n",
    "        final_outputs = self.sigmoid_activation(final_inputs) # signals from final output layer \n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.Train the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.95      0.59      5961\n",
      "           1       0.47      0.03      0.06      7781\n",
      "\n",
      "    accuracy                           0.43     13742\n",
      "   macro avg       0.45      0.49      0.33     13742\n",
      "weighted avg       0.45      0.43      0.29     13742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Set the hyperparameters here ###\n",
    "epochs = 100 #100\n",
    "learning_rate = 0.01 #0.1\n",
    "hidden_nodes = 10 \n",
    "output_nodes = 1\n",
    "\n",
    "N_i = X_train.shape[1]\n",
    "network = MyNeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "for e in range(epochs):\n",
    "    network.train(X_under, y_under)\n",
    "    \n",
    "y_pred = network.run(X_test)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0) # if probability >= 0.5, it is 1, else 0\n",
    "\n",
    "print(classification_report(y_pred[0], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.SGD\n",
    "\n",
    "Stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_i = X_train.shape[1]\n",
    "network = MyNeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "random_row_idx = np.zeros(32)\n",
    "for e in range(epochs):\n",
    "    random_row_idx = np.random.choice(X_under.shape[0],size=32)\n",
    "    X_batch = X_under[random_row_idx,:]\n",
    "    y_batch = y_under[random_row_idx]\n",
    "    network.train(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.96      0.62      6398\n",
      "           1       0.47      0.03      0.06      7344\n",
      "\n",
      "    accuracy                           0.46     13742\n",
      "   macro avg       0.47      0.50      0.34     13742\n",
      "weighted avg       0.47      0.46      0.32     13742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = network.run(X_test)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0) # if probability >= 0.5, it is 1, else 0\n",
    "print(classification_report(y_pred[0], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see Neural network and its results are pretty much consistent with our ML models in this notebook and to previous notebooks' results. Idea was to see if any better can be obtained. Additionally, I wanted to show that I have one additional tool in my bag.\n",
    "\n",
    "From here, we have seen that models are not making much improvement. In next notebook , I ll explain what are evaluation matrics are, what they mean to our problem, also I ll tune ML model and give more suggestions to improve results by model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldvDx7DPUErC"
   },
   "source": [
    "**END OF NOTEBOOK 4**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3.Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
